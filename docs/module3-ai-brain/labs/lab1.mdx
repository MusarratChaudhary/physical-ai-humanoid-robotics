---
title: "Lab 1: Setting Up Isaac Sim for Humanoid Robot Simulation"
sidebar_label: "Lab 1: Isaac Sim Setup"
---

# Lab 1: Setting Up Isaac Sim for Humanoid Robot Simulation

In this lab, you'll set up NVIDIA Isaac Sim for humanoid robot simulation, create a photorealistic environment, and generate synthetic data for AI training. This foundational exercise will prepare you for more advanced AI robotics applications.

## Learning Objectives

By the end of this lab, you will be able to:
- Install and configure NVIDIA Isaac Sim
- Import a humanoid robot model into Isaac Sim
- Create a photorealistic simulation environment
- Configure sensors for data collection
- Generate synthetic datasets for AI training
- Validate the simulation setup

## Prerequisites

Before starting this lab, ensure you have:
- NVIDIA GPU with CUDA support (RTX series recommended)
- NVIDIA Omniverse installed
- Isaac Sim installed and licensed
- Basic understanding of USD (Universal Scene Description)
- Experience with 3D modeling software (Blender/Maya) is helpful

## Step 1: Installing Isaac Sim

First, let's ensure Isaac Sim is properly installed:

1. Download Isaac Sim from the NVIDIA Developer website
2. Install Omniverse Launcher if not already installed
3. Add Isaac Sim to your Omniverse apps through the launcher
4. Launch Isaac Sim and verify the installation

## Step 2: Understanding Isaac Sim Interface

Familiarize yourself with the Isaac Sim interface:

1. **Viewport**: Main 3D scene view
2. **Stage Panel**: USD scene hierarchy
3. **Property Panel**: Selected object properties
4. **Layers Panel**: USD layer management
5. **Timeline**: Animation and simulation controls

## Step 3: Creating a Humanoid Robot Scene

Let's create a scene with a humanoid robot:

1. Open Isaac Sim
2. Create a new stage (File → New Stage)
3. Import a humanoid robot model:
   - Go to Window → Content Browser
   - Navigate to the robot assets folder
   - Drag and drop a humanoid robot model into the scene
   - Alternatively, import a URDF file via Isaac Sim's URDF importer extension

### Robot Import Example (Python API):
```python
import omni
from omni.isaac.core.utils.nucleus import get_assets_root_path
from omni.isaac.core.utils.stage import add_reference_to_stage
from pxr import UsdGeom

# Get the asset root path
assets_root_path = get_assets_root_path()

# Add a robot to the stage
if assets_root_path:
    robot_asset_path = assets_root_path + "/Isaac/Robots/Franka/franka_instanceable.usd"
    add_reference_to_stage(usd_path=robot_asset_path, prim_path="/World/Robot")
else:
    print("Could not find Isaac Sim assets. Did you install Isaac Sim correctly?")

# Set up the stage units
stage = omni.usd.get_context().get_stage()
UsdGeom.SetStageMetersPerUnit(stage, 0.01)
UsdGeom.SetStageUpAxis(stage, UsdGeom.Tokens.z)
```

## Step 4: Setting Up a Photorealistic Environment

Create a realistic environment for your humanoid robot:

1. Add a ground plane:
   - Create → Prim → Xform → GroundPlane
   - Set dimensions (e.g., 10m x 10m)
   - Add a realistic material (grass, concrete, etc.)

2. Add lighting:
   - Create → Light → DistantLight
   - Adjust intensity and direction
   - Optionally add dome lights for ambient lighting

3. Add environmental objects:
   - Tables, chairs, walls, or other relevant objects
   - Use the Content Browser to find assets
   - Position objects to create a realistic scene

### Environment Setup (Python API):
```python
from omni.isaac.core.utils.prims import create_primitive
from omni.isaac.core.utils.stage import set_stage_units
from omni.isaac.core.utils.carb import set_carb_setting
from omni.isaac.core import World
from omni.isaac.core.utils.viewports import set_active_viewport_camera_path
import carb

# Set stage units to meters
set_stage_units("meter")

# Create ground plane
create_primitive(
    prim_path="/World/GroundPlane",
    primitive_props={"size": 10, "color": [0.2, 0.2, 0.2]},
    usd_path="/Isaac/Props/Prims/ground_plane.usd"
)

# Create a table
table = create_primitive(
    prim_path="/World/Table",
    primitive_type="Cuboid",
    scale=[1.0, 0.8, 0.8],
    position=[1.0, 0.0, 0.4],
    color=[0.8, 0.7, 0.6]
)

# Create a distant light
from omni.isaac.core.utils.prims import define_prim
from pxr import UsdLux
light_prim = define_prim("/World/DistantLight", "DistantLight")
light_prim.GetAttribute("inputs:intensity").Set(3000)
light_prim.GetAttribute("inputs:angle").Set(0.5)
```

## Step 5: Configuring Sensors for Data Collection

Add sensors to your humanoid robot for perception tasks:

1. Add a camera to the robot's head:
   - Select the robot's head link
   - Create → Replicator → Annotated Camera
   - Configure camera properties (resolution, FOV, etc.)

2. Add a LiDAR sensor:
   - Select an appropriate mounting point on the robot
   - Create → Isaac Sensors → Isaac ROS RTX LiDAR
   - Configure LiDAR parameters (range, resolution, etc.)

### Sensor Configuration (Python API):
```python
from omni.isaac.sensor import Camera, RotatingLidarSensor
from omni.isaac.range_sensor import _range_sensor
import omni.kit.commands
import numpy as np

# Add a camera to the robot
camera = Camera(
    prim_path="/World/Robot/head/camera",
    frequency=20,
    resolution=(640, 480),
    position=np.array([0.0, 0.0, 0.1]),
    orientation=np.array([0, 0, 0, 1])
)

# Add a LiDAR sensor
lidar = RotatingLidarSensor(
    prim_path="/World/Robot/base_scan",
    translation=np.array([0.0, 0.0, 0.5]),
    orientation=np.array([0, 0, 0, 1]),
    config="ShortRange",
    rotation_frequency=10,
    samples_per_scan=1080,
    update_frequency=10
)
```

## Step 6: Setting Up Synthetic Data Generation

Configure Isaac Sim for synthetic data generation:

1. Enable the Synthetic Data Extension:
   - Window → Extensions → Isaac Utils → Synthetic Data Exporter

2. Configure data export settings:
   - Select the type of data to export (images, annotations, etc.)
   - Set export format (COCO, KITTI, etc.)
   - Define export directory

### Domain Randomization Setup:
```python
import omni.replicator.core as rep

# Enable replicator
with rep.new_layer():
    # Define randomization parameters
    def randomize_lighting():
        lights = rep.get.light()
        with lights.randomize.light_brightness(range=(1000, 5000), distribution="uniform"):
            with lights.randomize.color(temp_range=(5000, 7000)):
                yield
    
    def randomize_objects():
        # Randomize object positions
        objects = rep.get.prim_at_path("/World/Table")
        with objects.randomize.position(
            scale_position_distribution=rep.distribution.uniform((-2, -2, 0), (2, 2, 0))
        ):
            yield
    
    def randomize_materials():
        # Randomize material properties
        materials = rep.get.material()
        with materials.randomize.diffuse_reflection_strength(
            factor=rep.distribution.clamp(rep.distribution.normal(0.8, 0.1), 0, 1)
        ):
            yield
    
    # Register triggers
    rep.randomizer.register("randomize_lighting", randomize_lighting)
    rep.randomizer.register("randomize_objects", randomize_objects)
    rep.randomizer.register("randomize_materials", randomize_materials)
    
    # Create schedule
    trigger = rep.trigger.on_frame(num_frames=1)
    rep.randomizer.schedule(trigger)
```

## Step 7: Running the Simulation

1. Press the Play button to start the simulation
2. Verify that the robot appears in the scene
3. Check that sensors are functioning correctly
4. Use the Isaac Sim viewer to visualize sensor data

### Basic Simulation Control:
```python
from omni.isaac.core import World
import asyncio

# Create world instance
my_world = World(stage_units_in_meters=1.0)

# Add your robot to the world
# (Assuming you have a robot class defined)
# my_robot = MyRobot(prim_path="/World/Robot", name="my_robot")
# my_world.scene.add(my_robot)

# Reset the world
my_world.reset()

# Run simulation steps
for i in range(1000):
    my_world.step(render=True)
    
    # Get sensor data periodically
    if i % 100 == 0:
        # Process sensor data here
        print(f"Simulation step: {i}")
```

## Step 8: Generating Synthetic Datasets

1. Configure the Synthetic Data Exporter:
   - Select the camera sensor for image capture
   - Choose annotation types (bounding boxes, segmentation masks, etc.)
   - Set the number of frames to capture

2. Run the domain randomization:
   - Execute the randomization script
   - Capture multiple variations of the scene
   - Export the synthetic dataset

### Data Export Example:
```python
import omni.syntheticdata as sd
import numpy as np
import PIL.Image

# Capture RGB image
rgb_tensor = sd.acquire_rgb_buffer("camera")
rgb_image = PIL.Image.fromarray((rgb_tensor * 255).astype(np.uint8))

# Capture semantic segmentation
seg_tensor = sd.acquire_segmentation_buffer("camera", dtype=np.uint8)
seg_image = PIL.Image.fromarray(seg_tensor)

# Capture depth data
depth_tensor = sd.acquire_depth_buffer("camera")
depth_image = PIL.Image.fromarray((depth_tensor * 255).astype(np.uint8))

# Save images
rgb_image.save("/path/to/output/rgb_image.png")
seg_image.save("/path/to/output/seg_image.png")
depth_image.save("/path/to/output/depth_image.png")
```

## Step 9: Validating the Setup

1. Check that the robot moves correctly in the simulation
2. Verify that sensors are producing expected data
3. Confirm that synthetic data generation is working
4. Test the domain randomization functionality

## Step 10: Creating a Custom Environment

Create a more complex environment for humanoid robot training:

1. Design a room with furniture
2. Add dynamic objects (moving obstacles)
3. Implement different lighting conditions
4. Create multiple scene variations

### Example Environment Script:
```python
from omni.isaac.core import World
from omni.isaac.core.utils.prims import create_primitive
from omni.isaac.core.utils.stage import add_reference_to_stage
from omni.isaac.core.utils.nucleus import get_assets_root_path
import numpy as np

def create_complex_room():
    """Create a complex indoor environment"""
    
    # Create walls
    wall_thickness = 0.1
    room_size = 5.0
    
    # Four walls
    create_primitive(
        prim_path="/World/Wall1",
        primitive_type="Cuboid",
        scale=[room_size, wall_thickness, 2.5],
        position=[0, -room_size/2, 1.25],
        color=[0.7, 0.7, 0.7]
    )
    
    create_primitive(
        prim_path="/World/Wall2",
        primitive_type="Cuboid",
        scale=[room_size, wall_thickness, 2.5],
        position=[0, room_size/2, 1.25],
        color=[0.7, 0.7, 0.7]
    )
    
    create_primitive(
        prim_path="/World/Wall3",
        primitive_type="Cuboid",
        scale=[wall_thickness, room_size, 2.5],
        position=[-room_size/2, 0, 1.25],
        color=[0.7, 0.7, 0.7]
    )
    
    create_primitive(
        prim_path="/World/Wall4",
        primitive_type="Cuboid",
        scale=[wall_thickness, room_size, 2.5],
        position=[room_size/2, 0, 1.25],
        color=[0.7, 0.7, 0.7]
    )
    
    # Add furniture
    create_primitive(
        prim_path="/World/Chair",
        primitive_type="Cuboid",
        scale=[0.5, 0.5, 0.8],
        position=[1.0, 1.0, 0.4],
        color=[0.5, 0.3, 0.1]
    )
    
    create_primitive(
        prim_path="/World/Table",
        primitive_type="Cuboid",
        scale=[1.0, 0.6, 0.7],
        position=[-1.0, -1.0, 0.35],
        color=[0.2, 0.6, 0.4]
    )

# Call the function to create the environment
create_complex_room()
```

## Lab Summary

In this lab, you:
- Set up Isaac Sim for humanoid robot simulation
- Created a photorealistic environment
- Configured sensors for perception tasks
- Implemented domain randomization for synthetic data generation
- Validated the simulation setup

## Troubleshooting Tips

1. If Isaac Sim fails to launch, ensure your GPU supports RTX and has the latest drivers
2. If sensors don't appear, check that Isaac Sensors extension is enabled
3. If domain randomization isn't working, verify that Replicator extension is enabled
4. If the robot doesn't move, check that the robot model has proper articulation bodies

## Next Steps

In the next lab, you'll explore Isaac ROS packages for hardware-accelerated perception and navigation, integrating your simulation with real robotics frameworks.