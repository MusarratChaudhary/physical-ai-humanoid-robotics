---
title: "Lab 1: Implementing Vision Systems with CLIP Integration"
sidebar_label: "Lab 1: Vision Systems"
---

# Lab 1: Implementing Vision Systems with CLIP Integration

In this lab, you'll implement a vision system for a VLA model using CLIP (Contrastive Language-Image Pre-training) to connect visual perception with language understanding. You'll learn to process visual input, extract meaningful features, and prepare the visual stream for integration with language and action components.

## Learning Objectives

By the end of this lab, you will be able to:
- Set up and use CLIP for vision-language integration
- Process visual input for VLA systems
- Extract and encode visual features for downstream processing
- Implement visual grounding techniques
- Create a basic vision pipeline for robotic applications

## Prerequisites

Before starting this lab, ensure you have:
- Python 3.8+ installed
- Basic understanding of deep learning concepts
- Familiarity with PyTorch
- Completed Module 1-3 of this course
- A webcam or image dataset for testing

## Step 1: Setting Up the Environment

First, let's install the necessary packages for our vision system:

```bash
pip install torch torchvision
pip install clip
pip install opencv-python
pip install pillow
pip install numpy
```

Now let's create a basic vision processing module:

### Vision Processing Module:
```python
import torch
import clip
from PIL import Image
import cv2
import numpy as np
import matplotlib.pyplot as plt

class VisionProcessor:
    def __init__(self, model_name="ViT-B/32"):
        """Initialize the vision processor with a CLIP model."""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model, self.preprocess = clip.load(model_name, device=self.device)
        self.model.eval()  # Set to evaluation mode
        
        print(f"Vision processor initialized on {self.device}")
        print(f"Model: {model_name}")
    
    def preprocess_image(self, image_path_or_array):
        """Preprocess an image for CLIP."""
        if isinstance(image_path_or_array, str):
            # Load image from file
            image = Image.open(image_path_or_array).convert("RGB")
        else:
            # Assume it's a numpy array or similar
            image = Image.fromarray(image_path_or_array.astype('uint8')).convert("RGB")
        
        # Preprocess for CLIP
        image_input = self.preprocess(image).unsqueeze(0).to(self.device)
        return image_input, image
    
    def encode_image(self, image_path_or_array):
        """Encode an image using CLIP."""
        image_input, original_image = self.preprocess_image(image_path_or_array)
        
        with torch.no_grad():
            image_features = self.model.encode_image(image_input)
            # Normalize features
            image_features /= image_features.norm(dim=-1, keepdim=True)
        
        return image_features, original_image
    
    def encode_text(self, texts):
        """Encode text using CLIP."""
        if isinstance(texts, str):
            texts = [texts]
        
        text_tokens = clip.tokenize(texts).to(self.device)
        
        with torch.no_grad():
            text_features = self.model.encode_text(text_tokens)
            # Normalize features
            text_features /= text_features.norm(dim=-1, keepdim=True)
        
        return text_features
    
    def compute_similarity(self, image_path_or_array, texts):
        """Compute similarity between image and texts."""
        image_features, _ = self.encode_image(image_path_or_array)
        text_features = self.encode_text(texts)
        
        # Compute cosine similarity
        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
        return similarity.cpu().numpy()[0]
    
    def find_best_match(self, image_path_or_array, candidate_texts):
        """Find the best matching text for an image."""
        similarities = self.compute_similarity(image_path_or_array, candidate_texts)
        best_match_idx = np.argmax(similarities)
        return candidate_texts[best_match_idx], similarities[best_match_idx], similarities

def main():
    # Initialize the vision processor
    vision_processor = VisionProcessor()
    
    # Example usage
    print("Vision processor initialized successfully!")
    
    # You can test with an image file if you have one:
    # image_path = "path/to/your/image.jpg"
    # best_match, confidence, all_similarities = vision_processor.find_best_match(
    #     image_path, 
    #     ["a photo of a dog", "a photo of a cat", "a photo of a car"]
    # )
    # print(f"Best match: '{best_match}' with confidence: {confidence:.2f}")

if __name__ == "__main__":
    main()
```

## Step 2: Implementing Visual Grounding

Now let's implement visual grounding techniques that will help our VLA system identify specific objects in images based on language descriptions:

### Visual Grounding Implementation:
```python
import torch
import clip
from PIL import Image
import cv2
import numpy as np
import matplotlib.pyplot as plt
from torchvision.ops import nms
import torchvision.transforms as T

class VisualGrounding:
    def __init__(self, model_name="ViT-B/32"):
        """Initialize visual grounding with CLIP model."""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model, self.preprocess = clip.load(model_name, device=self.device)
        self.model.eval()
        
        # Transformation for sliding window
        self.transform = T.Compose([
            T.Resize((224, 224)),
            T.ToTensor(),
            T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], 
                       std=[0.26862954, 0.26130258, 0.27577711])
        ])
    
    def sliding_window_search(self, image_path, target_text, window_size=(224, 224), stride=112):
        """Perform sliding window search to find regions matching the target text."""
        original_image = Image.open(image_path).convert("RGB")
        img_array = np.array(original_image)
        h, w, _ = img_array.shape
        
        # Prepare target text
        text_features = self.encode_text(target_text)
        
        # Store results
        regions = []
        scores = []
        boxes = []
        
        # Slide window across image
        for y in range(0, h - window_size[1], stride):
            for x in range(0, w - window_size[0], stride):
                # Extract region
                region = img_array[y:y+window_size[1], x:x+window_size[0]]
                region_pil = Image.fromarray(region)
                
                # Preprocess region
                region_tensor = self.transform(region_pil).unsqueeze(0).to(self.device)
                
                # Encode region
                with torch.no_grad():
                    region_features = self.model.encode_image(region_tensor)
                    region_features /= region_features.norm(dim=-1, keepdim=True)
                
                # Compute similarity
                similarity = (region_features @ text_features.T).item()
                
                regions.append(region)
                scores.append(similarity)
                boxes.append([x, y, x + window_size[0], y + window_size[1]])
        
        # Convert to tensors for NMS
        boxes_tensor = torch.tensor(boxes, dtype=torch.float32)
        scores_tensor = torch.tensor(scores, dtype=torch.float32)
        
        # Apply NMS to remove overlapping boxes
        keep_indices = nms(boxes_tensor, scores_tensor, iou_threshold=0.5)
        
        # Filter results
        filtered_boxes = [boxes[i] for i in keep_indices]
        filtered_scores = [scores[i] for i in keep_indices]
        
        return filtered_boxes, filtered_scores, original_image
    
    def encode_text(self, text):
        """Encode text using CLIP."""
        text_tokens = clip.tokenize([text]).to(self.device)
        
        with torch.no_grad():
            text_features = self.model.encode_text(text_tokens)
            text_features /= text_features.norm(dim=-1, keepdim=True)
        
        return text_features
    
    def visualize_grounding(self, image, boxes, scores, title="Visual Grounding Results"):
        """Visualize the grounding results."""
        img_with_boxes = np.array(image.copy())
        
        for i, (box, score) in enumerate(zip(boxes, scores)):
            x1, y1, x2, y2 = box
            # Draw rectangle
            cv2.rectangle(img_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)
            # Add score text
            cv2.putText(img_with_boxes, f"{score:.2f}", 
                       (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
        
        # Display image
        plt.figure(figsize=(12, 8))
        plt.imshow(img_with_boxes)
        plt.title(title)
        plt.axis('off')
        plt.show()

def main():
    # Initialize visual grounding
    grounding = VisualGrounding()
    
    # Example usage (uncomment and provide an image path to test)
    # boxes, scores, image = grounding.sliding_window_search(
    #     "path/to/your/image.jpg", 
    #     "person wearing red shirt"
    # )
    # grounding.visualize_grounding(image, boxes, scores)
    
    print("Visual grounding module initialized successfully!")

if __name__ == "__main__":
    main()
```

## Step 3: Creating a Complete Vision Pipeline

Let's create a complete vision pipeline that integrates the components we've developed:

### Complete Vision Pipeline:
```python
import torch
import clip
from PIL import Image
import cv2
import numpy as np
import matplotlib.pyplot as plt
from torchvision.ops import nms
import time

class VisionPipeline:
    def __init__(self, model_name="ViT-B/32"):
        """Initialize the complete vision pipeline."""
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model, self.preprocess = clip.load(model_name, device=self.device)
        self.model.eval()
        
        print(f"Vision pipeline initialized on {self.device}")
        print(f"Model: {model_name}")
    
    def process_frame(self, frame):
        """Process a single video frame."""
        # Convert frame to PIL Image
        pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        
        # Preprocess for CLIP
        image_input = self.preprocess(pil_image).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            image_features = self.model.encode_image(image_input)
            image_features /= image_features.norm(dim=-1, keepdim=True)
        
        return image_features, pil_image
    
    def classify_scene(self, frame, candidate_labels):
        """Classify the scene based on candidate labels."""
        image_features, _ = self.process_frame(frame)
        
        # Encode text labels
        text_tokens = clip.tokenize(candidate_labels).to(self.device)
        
        with torch.no_grad():
            text_features = self.model.encode_text(text_tokens)
            text_features /= text_features.norm(dim=-1, keepdim=True)
        
        # Compute similarities
        similarities = (100.0 * image_features @ text_features.T).softmax(dim=-1)
        probabilities = similarities.cpu().numpy()[0]
        
        # Get top predictions
        top_indices = np.argsort(probabilities)[::-1]
        
        results = []
        for idx in top_indices:
            results.append({
                'label': candidate_labels[idx],
                'probability': probabilities[idx]
            })
        
        return results
    
    def detect_objects(self, frame, object_prompts, threshold=0.1):
        """Detect specific objects in the frame."""
        # This is a simplified version - a full implementation would use
        # sliding window or segmentation techniques
        image_features, pil_image = self.process_frame(frame)
        
        # Encode object prompts
        text_tokens = clip.tokenize(object_prompts).to(self.device)
        
        with torch.no_grad():
            text_features = self.model.encode_text(text_tokens)
            text_features /= text_features.norm(dim=-1, keepdim=True)
        
        # Compute similarities
        similarities = (100.0 * image_features @ text_features.T).softmax(dim=-1)
        probabilities = similarities.cpu().numpy()[0]
        
        # Filter based on threshold
        detected_objects = []
        for i, prob in enumerate(probabilities):
            if prob > threshold:
                detected_objects.append({
                    'object': object_prompts[i],
                    'confidence': prob
                })
        
        return detected_objects
    
    def run_real_time_demo(self, labels, objects):
        """Run a real-time demo using webcam."""
        cap = cv2.VideoCapture(0)
        
        if not cap.isOpened():
            print("Error: Could not open webcam.")
            return
        
        print("Starting real-time demo. Press 'q' to quit.")
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # Process frame
            start_time = time.time()
            
            # Classify scene
            scene_classification = self.classify_scene(frame, labels)
            
            # Detect objects
            detected_objects = self.detect_objects(frame, objects)
            
            processing_time = time.time() - start_time
            
            # Display results on frame
            # Scene classification
            cv2.putText(frame, f"Scene: {scene_classification[0]['label']} ({scene_classification[0]['probability']:.2f})", 
                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            
            # Detected objects
            for i, obj in enumerate(detected_objects[:3]):  # Show top 3
                cv2.putText(frame, f"Obj: {obj['object']} ({obj['confidence']:.2f})", 
                           (10, 60 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
            
            # Processing time
            cv2.putText(frame, f"Time: {processing_time:.3f}s", 
                       (10, frame.shape[0]-20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)
            
            # Show frame
            cv2.imshow('VLA Vision Pipeline', frame)
            
            # Break on 'q' key press
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
        
        cap.release()
        cv2.destroyAllWindows()

def main():
    # Initialize the vision pipeline
    pipeline = VisionPipeline()
    
    # Define labels and objects for demonstration
    scene_labels = [
        "indoor office", "outdoor street", "kitchen", 
        "living room", "bedroom", "garden"
    ]
    
    object_prompts = [
        "person", "chair", "table", "computer", 
        "plant", "book", "cup", "phone"
    ]
    
    print("Vision pipeline ready!")
    print("Available scene labels:", scene_labels)
    print("Available object prompts:", object_prompts)
    
    # Uncomment the next line to run the real-time demo
    # pipeline.run_real_time_demo(scene_labels, object_prompts)

if __name__ == "__main__":
    main()
```

## Step 4: Integrating with ROS 2

Now let's create a ROS 2 node that implements our vision pipeline:

### ROS 2 Vision Node:
```python
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from std_msgs.msg import String
from vision_msgs.msg import Detection2D, Detection2DArray, ObjectHypothesisWithPose
from cv_bridge import CvBridge
import torch
import clip
import cv2
import numpy as np

class VLAVisionNode(Node):
    def __init__(self):
        super().__init__('vla_vision_node')
        
        # Initialize CLIP model
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model, self.preprocess = clip.load("ViT-B/32", device=self.device)
        self.model.eval()
        
        # Initialize CV bridge
        self.cv_bridge = CvBridge()
        
        # Create subscriber for camera images
        self.image_sub = self.create_subscription(
            Image,
            '/camera/image_raw',
            self.image_callback,
            10
        )
        
        # Create publishers for detections and classifications
        self.detection_pub = self.create_publisher(Detection2DArray, '/vision/detections', 10)
        self.classification_pub = self.create_publisher(String, '/vision/classification', 10)
        
        # Define object categories for detection
        self.object_categories = [
            "person", "chair", "table", "computer", 
            "plant", "book", "cup", "phone", "bottle"
        ]
        
        self.get_logger().info('VLA Vision Node initialized')

    def image_callback(self, msg):
        """Process incoming image and perform vision tasks."""
        try:
            # Convert ROS image to OpenCV image
            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")
            
            # Perform scene classification
            scene_classification = self.classify_scene(cv_image)
            
            # Perform object detection
            detections = self.detect_objects(cv_image)
            
            # Publish results
            self.publish_detections(detections, msg.header)
            self.publish_classification(scene_classification, msg.header)
            
        except Exception as e:
            self.get_logger().error(f'Error processing image: {str(e)}')

    def classify_scene(self, cv_image):
        """Classify the scene in the image."""
        # Define scene categories
        scene_categories = [
            "indoor office", "outdoor street", "kitchen", 
            "living room", "bedroom", "garden"
        ]
        
        # Convert image to PIL and preprocess
        pil_image = Image.fromarray(cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB))
        image_input = self.preprocess(pil_image).unsqueeze(0).to(self.device)
        
        # Encode image
        with torch.no_grad():
            image_features = self.model.encode_image(image_input)
            image_features /= image_features.norm(dim=-1, keepdim=True)
        
        # Encode scene categories
        text_tokens = clip.tokenize(scene_categories).to(self.device)
        
        with torch.no_grad():
            text_features = self.model.encode_text(text_tokens)
            text_features /= text_features.norm(dim=-1, keepdim=True)
        
        # Compute similarities
        similarities = (100.0 * image_features @ text_features.T).softmax(dim=-1)
        probabilities = similarities.cpu().numpy()[0]
        
        # Get top scene classification
        top_idx = np.argmax(probabilities)
        return scene_categories[top_idx]

    def detect_objects(self, cv_image):
        """Detect objects in the image."""
        # Convert image to PIL and preprocess
        pil_image = Image.fromarray(cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB))
        image_input = self.preprocess(pil_image).unsqueeze(0).to(self.device)
        
        # Encode image
        with torch.no_grad():
            image_features = self.model.encode_image(image_input)
            image_features /= image_features.norm(dim=-1, keepdim=True)
        
        # Encode object categories
        text_tokens = clip.tokenize(self.object_categories).to(self.device)
        
        with torch.no_grad():
            text_features = self.model.encode_text(text_tokens)
            text_features /= text_features.norm(dim=-1, keepdim=True)
        
        # Compute similarities
        similarities = (100.0 * image_features @ text_features.T).softmax(dim=-1)
        probabilities = similarities.cpu().numpy()[0]
        
        # Create detection results
        detections = []
        for i, prob in enumerate(probabilities):
            if prob > 0.1:  # Threshold for detection
                detection = {
                    'category': self.object_categories[i],
                    'confidence': float(prob)
                }
                detections.append(detection)
        
        return detections

    def publish_detections(self, detections, header):
        """Publish object detections."""
        detections_msg = Detection2DArray()
        detections_msg.header = header
        
        for detection in detections:
            detection_msg = Detection2D()
            
            # For simplicity, we're not setting bounding box coordinates
            # In a real implementation, you would use a proper object detector
            # to get bounding box information
            
            # Set category and confidence
            hypothesis = ObjectHypothesisWithPose()
            hypothesis.id = detection['category']
            hypothesis.score = detection['confidence']
            detection_msg.results.append(hypothesis)
            
            detections_msg.detections.append(detection_msg)
        
        self.detection_pub.publish(detections_msg)

    def publish_classification(self, classification, header):
        """Publish scene classification."""
        classification_msg = String()
        classification_msg.data = classification
        self.classification_pub.publish(classification_msg)

def main(args=None):
    rclpy.init(args=args)
    
    vision_node = VLAVisionNode()
    
    try:
        rclpy.spin(vision_node)
    except KeyboardInterrupt:
        pass
    finally:
        vision_node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Step 5: Testing the Vision System

Let's create a test script to validate our vision system:

### Vision System Test:
```python
import torch
import clip
from PIL import Image
import cv2
import numpy as np
from vision_processor import VisionProcessor
from visual_grounding import VisualGrounding
from vision_pipeline import VisionPipeline

def test_vision_processor():
    """Test the basic vision processor."""
    print("Testing Vision Processor...")
    
    # Initialize processor
    processor = VisionProcessor()
    
    # Test with sample texts
    sample_texts = ["a photo of a dog", "a photo of a cat", "a photo of a car"]
    
    # Since we don't have a specific image, we'll just test the text encoding
    text_features = processor.encode_text(sample_texts)
    print(f"Encoded {len(sample_texts)} texts with shape: {text_features.shape}")
    
    print("Vision Processor test completed.\n")

def test_visual_grounding():
    """Test the visual grounding module."""
    print("Testing Visual Grounding...")
    
    # Initialize grounding
    grounding = VisualGrounding()
    
    # Test text encoding
    text_features = grounding.encode_text("person wearing red shirt")
    print(f"Encoded text with shape: {text_features.shape}")
    
    print("Visual Grounding test completed.\n")

def test_vision_pipeline():
    """Test the complete vision pipeline."""
    print("Testing Vision Pipeline...")
    
    # Initialize pipeline
    pipeline = VisionPipeline()
    
    # Test with sample labels and objects
    labels = ["indoor office", "outdoor street", "kitchen"]
    objects = ["person", "chair", "table"]
    
    print(f"Scene labels: {labels}")
    print(f"Object prompts: {objects}")
    
    print("Vision Pipeline test completed.\n")

def main():
    print("Starting Vision System Tests...\n")
    
    test_vision_processor()
    test_visual_grounding()
    test_vision_pipeline()
    
    print("All vision system tests completed!")

if __name__ == "__main__":
    main()
```

## Step 6: Running the Vision System

1. First, run the basic vision processor test:
   ```bash
   python vision_test.py
   ```

2. To run the real-time demo (if you have a webcam):
   ```bash
   python vision_pipeline.py
   ```

3. To run the ROS 2 node (after setting up ROS 2):
   ```bash
   ros2 run your_package_name vla_vision_node
   ```

## Step 7: Evaluating Performance

Monitor the performance of your vision system:

1. Check processing time per frame:
   ```bash
   # In your terminal while running the real-time demo
   # Look at the "Time" displayed on the video feed
   ```

2. Evaluate accuracy with a test dataset:
   ```python
   # Example evaluation code
   import os
   from vision_processor import VisionProcessor
   
   processor = VisionProcessor()
   
   # Test on a directory of images with known labels
   test_dir = "path/to/test/images"
   correct_predictions = 0
   total_predictions = 0
   
   for image_file in os.listdir(test_dir):
       if image_file.endswith(('.jpg', '.jpeg', '.png')):
           image_path = os.path.join(test_dir, image_file)
           
           # Get predicted label (this would depend on your specific setup)
           # For example, if you know the true label is in the filename:
           true_label = image_file.split('_')[0]  # Assuming format: label_description.jpg
           
           candidates = ["dog", "cat", "car", "person", "chair", "table"]  # Example candidates
           predicted_label, confidence, _ = processor.find_best_match(image_path, candidates)
           
           if predicted_label.split()[2] == true_label:  # Adjust based on your candidate format
               correct_predictions += 1
           total_predictions += 1
   
   accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
   print(f"Accuracy: {accuracy:.2f}")
   ```

## Lab Summary

In this lab, you:
- Implemented a vision processing system using CLIP for vision-language integration
- Created visual grounding techniques to identify specific objects based on language descriptions
- Built a complete vision pipeline for robotic applications
- Integrated the vision system with ROS 2 for robotic applications
- Tested and evaluated the performance of your vision system

## Troubleshooting Tips

1. If you get CUDA memory errors, try using the CPU version by setting device to "cpu"
2. If CLIP model loading fails, ensure you have an internet connection for the initial download
3. If webcam access fails, check permissions and ensure no other applications are using the camera
4. If processing is too slow, consider using a lighter CLIP model like "RN50" instead of "ViT-B/32"

## Next Steps

In the next lab, you'll integrate language understanding capabilities using OpenAI Whisper and LLMs to create a complete voice-to-action pipeline.